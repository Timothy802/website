<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nlp on Tianqi</title>
    <link>https://Timothy802.github.io/website/tags/nlp/</link>
    <description>Recent content in nlp on Tianqi</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>tianqi93@connect.hku.hk (Tianqi)</managingEditor>
    <webMaster>tianqi93@connect.hku.hk (Tianqi)</webMaster>
    <lastBuildDate>Mon, 21 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://Timothy802.github.io/website/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Word2Vec Model</title>
      <link>https://Timothy802.github.io/website/post/2020-12-21-word2vec-model/</link>
      <pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate>
      <author>tianqi93@connect.hku.hk (Tianqi)</author>
      <guid>https://Timothy802.github.io/website/post/2020-12-21-word2vec-model/</guid>
      <description>Introduction Word2Vec was introduced by a team of researchers at Google in 2013, which is one of the most widely used neural network language models.
The underlying assumption of Word2Vec is that two words sharing similar contexts also share a similar meaning (Distributional Hypothesis) and consequently a similar vector representation from the model. For instance, &amp;ldquo;dog&amp;rdquo;, &amp;ldquo;puppy&amp;rdquo; and &amp;ldquo;pup&amp;rdquo; are often used in similar situations, with similar surrounding words like &amp;ldquo;good&amp;rdquo;, &amp;ldquo;fluffy&amp;rdquo; or &amp;ldquo;cute&amp;rdquo;, and according to Word2Vec they will therefore share a similar vector representation.</description>
    </item>
    
  </channel>
</rss>
